#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
thann.py
--------
A production-ready example of a Toroidal Hive (THANN) prototype using
smaller, reliable Hugging Face models for text, fill-mask, and image tasks,
and now integrated dataset loaders for richer inputs.

New Features:
  1. ResNet (microsoft/resnet-50) pipeline for image classification.
  2. Gradio UI to demo text generation and ResNet-based image classification.
  3. Integrated dataset loaders: 'custom', 'coco', and 'shapenet'.

Requirements:
  pip install torch transformers sentence-transformers gradio numpy rembg xatlas pillow
"""

import argparse
import torch
import numpy as np
import glob
import os

# Hugging Face Transformers
from transformers import (
    pipeline,
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModelForMaskedLM,
    AutoImageProcessor,
    AutoModelForImageClassification,
    AutoProcessor,
    AutoModelForZeroShotImageClassification,
    AutoModelForSeq2SeqLM
)

# Sentence Transformers (for embeddings/similarity)
from sentence_transformers import SentenceTransformer

# Custom modules from THANN (assumed available)
# These modules should implement the TSR system, background removal, video saving, and texture baking.
from tsr.system import TSR
from tsr.utils import remove_background, resize_foreground, save_video
from tsr.bake_texture import bake_texture

##############################################################################
# 1. Toroidal Hive Model (Conceptual Example)
##############################################################################

class ToroidalHiveModel(torch.nn.Module):
    """
    A conceptual neural network illustrating a 'toroidal' feedback loop.
    In a real THANN system, you'd add:
      - Self-healing or error correction layers
      - Emotional weighting or adaptive learning rates
      - Possibly quantum modules (if using Qiskit, etc.)
      - Swarm-based multi-agent logic

    Here, it's an LSTM with a simple feedback transform for demonstration.
    """
    def __init__(self, hidden_size=256, vocab_size=30522):
        super(ToroidalHiveModel, self).__init__()
        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=hidden_size)
        self.lstm = torch.nn.LSTM(hidden_size, hidden_size, batch_first=True)
        self.classifier = torch.nn.Linear(hidden_size, vocab_size)

        # Example of a feedback transformation for post-processing logits
        self.feedback_transform = torch.nn.Linear(vocab_size, hidden_size)

    def forward(self, input_ids, hidden=None):
        """
        Forward pass with a basic feedback loop:
        1) Input embeddings
        2) LSTM
        3) Classifier -> vocab logits
        4) Convert the average softmax logits back to hidden space
        """
        # 1. Embedding
        emb = self.embedding(input_ids)
        # 2. LSTM
        output, (h_n, c_n) = self.lstm(emb, hidden)
        # 3. Classifier
        logits = self.classifier(output)
        # 4. Simple feedback
        feedback = torch.softmax(logits, dim=-1).mean(dim=1)  # average over time dimension
        feedback_hidden = self.feedback_transform(feedback)
        # You could feed 'feedback_hidden' back into the LSTM or a specialized module.
        return logits, (h_n, c_n)

##############################################################################
# 2. Multimodal Pipelines Using Smaller, Real Models (with ResNet added)
##############################################################################

def create_multimodal_pipelines():
    """
    Returns a dictionary of Hugging Face pipeline objects for:
      - Text Generation (DistilGPT2)
      - Fill-Mask (BERT base)
      - Image Classification (ViT base)
      - Zero-Shot Image Classification (CLIP base)
      - Text2Text Generation (FLAN-T5-Small)
      - ResNet Image Classification (microsoft/resnet-50)
    Each model is relatively small, so you avoid long downloads or timeouts.
    """
    pipelines_dict = {}

    # TEXT GENERATION: DistilGPT2 (~500 MB)
    try:
        text_gen_tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
        text_gen_model = AutoModelForCausalLM.from_pretrained("distilgpt2")
        pipelines_dict["text_generation"] = pipeline(
            "text-generation",
            model=text_gen_model,
            tokenizer=text_gen_tokenizer
        )
    except Exception as e:
        print(f"[Warning] Failed to load text-generation (distilgpt2): {e}")

    # FILL-MASK: BERT base (~420 MB)
    try:
        fill_mask_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        fill_mask_model = AutoModelForMaskedLM.from_pretrained("bert-base-uncased")
        pipelines_dict["fill_mask"] = pipeline(
            "fill-mask",
            model=fill_mask_model,
            tokenizer=fill_mask_tokenizer
        )
    except Exception as e:
        print(f"[Warning] Failed to load fill-mask (bert-base-uncased): {e}")

    # IMAGE CLASSIFICATION: ViT base (~330 MB)
    try:
        image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
        image_model = AutoModelForImageClassification.from_pretrained("google/vit-base-patch16-224")
        pipelines_dict["image_classification"] = pipeline(
            "image-classification",
            model=image_model,
            feature_extractor=image_processor
        )
    except Exception as e:
        print(f"[Warning] Failed to load image-classification (vit-base): {e}")

    # ZERO-SHOT IMAGE CLASSIFICATION: CLIP base (~500 MB)
    try:
        zero_shot_processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")
        zero_shot_model = AutoModelForZeroShotImageClassification.from_pretrained("openai/clip-vit-base-patch32")
        pipelines_dict["zero_shot_image"] = pipeline(
            "zero-shot-image-classification",
            model=zero_shot_model,
            feature_extractor=zero_shot_processor
        )
    except Exception as e:
        print(f"[Warning] Failed to load zero-shot-image (clip-vit-base-patch32): {e}")

    # TEXT2TEXT GENERATION: FLAN-T5-Small (~300 MB)
    try:
        text2text_tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")
        text2text_model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")
        pipelines_dict["text2text_generation"] = pipeline(
            "text2text-generation",
            model=text2text_model,
            tokenizer=text2text_tokenizer
        )
    except Exception as e:
        print(f"[Warning] Failed to load text2text-generation (flan-t5-small): {e}")

    # RESNET IMAGE CLASSIFICATION: microsoft/resnet-50
    try:
        resnet_processor = AutoImageProcessor.from_pretrained("microsoft/resnet-50")
        resnet_model = AutoModelForImageClassification.from_pretrained("microsoft/resnet-50")
        pipelines_dict["resnet_classification"] = pipeline(
            "image-classification",
            model=resnet_model,
            feature_extractor=resnet_processor
        )
    except Exception as e:
        print(f"[Warning] Failed to load resnet classification (microsoft/resnet-50): {e}")

    return pipelines_dict

##############################################################################
# 3. Sentence Embeddings Example
##############################################################################

def example_sentence_embeddings():
    """
    Demonstrates how to compute embeddings with SentenceTransformer
    (all-MiniLM-L6-v2) and produce a simple dot-product similarity matrix.
    """
    model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    sentences = [
        "That is a happy person",
        "That is a happy dog",
        "That is a very happy person",
        "Today is a sunny day"
    ]
    embeddings = model.encode(sentences)
    # Dot product similarity matrix
    sim_matrix = np.inner(embeddings, embeddings)
    return sentences, sim_matrix

##############################################################################
# 4. Gradio UI
##############################################################################

def run_gradio_ui(pipelines_dict):
    """
    Launches a simple Gradio app demonstrating:
      1) Text generation with DistilGPT2 (if loaded).
      2) ResNet-50 image classification (if loaded).
    """
    import gradio as gr

    # Text generation function
    def generate_text(prompt):
        if "text_generation" not in pipelines_dict:
            return "Text generation pipeline not loaded."
        result = pipelines_dict["text_generation"](prompt, max_length=40, do_sample=True)
        return result[0]["generated_text"]

    # ResNet image classification function
    def classify_image(image):
        if "resnet_classification" not in pipelines_dict:
            return "ResNet classification pipeline not loaded."
        results = pipelines_dict["resnet_classification"](image)
        return results

    with gr.Blocks() as demo:
        gr.Markdown("# THANN Demo: Text Generation & ResNet Classification")

        with gr.Tab("Text Generation"):
            text_input = gr.Textbox(lines=3, label="Enter your prompt:")
            text_output = gr.Textbox(label="Generated Text")
            generate_button = gr.Button("Generate")
            generate_button.click(fn=generate_text, inputs=text_input, outputs=text_output)

        with gr.Tab("ResNet Image Classification"):
            image_input = gr.Image(label="Upload an image", type="pil")
            image_output = gr.JSON(label="Classification Results")
            classify_button = gr.Button("Classify")
            classify_button.click(fn=classify_image, inputs=image_input, outputs=image_output)

        demo.launch()

##############################################################################
# 5. Dataset Loader Functions
##############################################################################

def load_custom_images(image_paths):
    """Load images from a list of provided file paths."""
    images = []
    for image_path in image_paths:
        image = np.array(Image.open(image_path).convert("RGB"))
        images.append(image)
    return images

def load_coco_dataset(dataset_dir):
    """
    Load images from a sample COCO dataset directory.
    This function assumes that dataset_dir contains JPEG/PNG images.
    """
    image_files = glob.glob(os.path.join(dataset_dir, "*.jpg")) + glob.glob(os.path.join(dataset_dir, "*.png"))
    if len(image_files) == 0:
        print(f"[Error] No images found in {dataset_dir}")
    images = []
    for file in image_files:
        image = np.array(Image.open(file).convert("RGB"))
        images.append(image)
    print(f"Loaded {len(images)} images from COCO dataset at {dataset_dir}.")
    return images

def load_shapenet_dataset(dataset_dir):
    """
    (Placeholder) Load images or 3D models from a ShapeNet dataset directory.
    In a full implementation, you would parse ShapeNet 3D models and/or their rendered views.
    Here, we load sample rendered images for demonstration.
    """
    image_files = glob.glob(os.path.join(dataset_dir, "*.jpg")) + glob.glob(os.path.join(dataset_dir, "*.png"))
    if len(image_files) == 0:
        print(f"[Error] No images found in {dataset_dir}")
    images = []
    for file in image_files:
        image = np.array(Image.open(file).convert("RGB"))
        images.append(image)
    print(f"Loaded {len(images)} sample rendered images from ShapeNet dataset at {dataset_dir}.")
    return images

##############################################################################
# 6. Main Script
##############################################################################

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--demo", action="store_true",
                        help="Run demonstration: pipelines + embeddings.")
    parser.add_argument("--ui", action="store_true",
                        help="Launch Gradio UI for text & image classification.")
    parser.add_argument("--dataset", type=str, default="custom", choices=["custom", "coco", "shapenet"],
                        help="Choose dataset mode: 'custom' for manual image paths; 'coco' or 'shapenet' for built-in dataset loading. Default: custom")
    parser.add_argument("--dataset-dir", type=str, default="",
                        help="Directory of the dataset to load if --dataset is not 'custom'.")
    parser.add_argument("image", type=str, nargs="*",
                        help="Path(s) to input image(s). Ignored if --dataset is specified.")
    args = parser.parse_args()

    # Create pipelines if demo or UI is used
    if args.demo or args.ui:
        pipelines_dict = create_multimodal_pipelines()

    # Dataset processing
    if args.dataset == "custom":
        if len(args.image) == 0:
            print("[Error] No image paths provided and dataset mode is 'custom'. Exiting.")
            exit(1)
        images = load_custom_images(args.image)
    elif args.dataset == "coco":
        if args.dataset_dir == "":
            print("[Error] Please provide --dataset-dir for the COCO dataset.")
            exit(1)
        images = load_coco_dataset(args.dataset_dir)
    elif args.dataset == "shapenet":
        if args.dataset_dir == "":
            print("[Error] Please provide --dataset-dir for the ShapeNet dataset.")
            exit(1)
        images = load_shapenet_dataset(args.dataset_dir)
    else:
        images = []

    # Set device
    device = "cuda:0"
    if not torch.cuda.is_available():
        device = "cpu"

    # Initialize TSR model
    print("Initializing model...")
    timer = Timer()
    timer.start("Initializing model")
    model = TSR.from_pretrained(
        "stabilityai/TripoSR",
        config_name="config.yaml",
        weight_name="model.ckpt",
    )
    model.renderer.set_chunk_size(8192)
    model.to(device)
    timer.end("Initializing model")

    # Process images (background removal & resizing if needed)
    print("Processing images...")
    timer.start("Processing images")
    processed_images = []
    # If background removal is enabled (assumed by default), apply it
    if not hasattr(args, "no_remove_bg") or not args.no_remove_bg:
        import rembg
        rembg_session = rembg.new_session()
        from PIL import Image
        for i, image in enumerate(images):
            # Convert numpy array to PIL image for processing
            pil_img = Image.fromarray(image)
            pil_img = remove_background(pil_img, rembg_session)
            pil_img = resize_foreground(pil_img, 0.85)
            # Normalize and reformat image
            np_img = np.array(pil_img).astype(np.float32) / 255.0
            np_img = np_img[:, :, :3] * np_img[:, :, 3:4] + (1 - np_img[:, :, 3:4]) * 0.5
            pil_img = Image.fromarray((np_img * 255.0).astype(np.uint8))
            # Save processed image to output folder
            output_dir = "output"
            os.makedirs(os.path.join(output_dir, str(i)), exist_ok=True)
            pil_img.save(os.path.join(output_dir, str(i), "input.png"))
            processed_images.append(pil_img)
    else:
        # If background removal is not required, directly convert images to PIL
        from PIL import Image
        processed_images = [Image.fromarray(image) for image in images]
    timer.end("Processing images")

    # Run model inference, rendering, mesh extraction and export results per image
    for i, image in enumerate(processed_images):
        print(f"Running image {i + 1} / {len(processed_images)} ...")
        timer.start("Running model")
        with torch.no_grad():
            scene_codes = model([image], device=device)
        timer.end("Running model")

        # Optional rendering
        if hasattr(args, "render") and args.render:
            timer.start("Rendering")
            render_images = model.render(scene_codes, n_views=30, return_type="pil")
            for ri, render_image in enumerate(render_images[0]):
                render_image.save(os.path.join("output", str(i), f"render_{ri:03d}.png"))
            save_video(render_images[0], os.path.join("output", str(i), "render.mp4"), fps=30)
            timer.end("Rendering")

        timer.start("Extracting mesh")
        meshes = model.extract_mesh(scene_codes, not getattr(args, "bake_texture", False), resolution=256)
        timer.end("Extracting mesh")

        out_mesh_path = os.path.join("output", str(i), f"mesh.obj")
        if getattr(args, "bake_texture", False):
            out_texture_path = os.path.join("output", str(i), "texture.png")
            timer.start("Baking texture")
            bake_output = bake_texture(meshes[0], model, scene_codes[0], 2048)
            timer.end("Baking texture")
            timer.start("Exporting mesh and texture")
            xatlas.export(out_mesh_path,
                          meshes[0].vertices[bake_output["vmapping"]],
                          bake_output["indices"],
                          bake_output["uvs"],
                          meshes[0].vertex_normals[bake_output["vmapping"]])
            from PIL import Image
            Image.fromarray((bake_output["colors"] * 255.0).astype(np.uint8))\
                 .transpose(Image.FLIP_TOP_BOTTOM)\
                 .save(out_texture_path)
            timer.end("Exporting mesh and texture")
        else:
            timer.start("Exporting mesh")
            meshes[0].export(out_mesh_path)
            timer.end("Exporting mesh")

    if args.demo:
        # Display demo information for pipelines and sentence embeddings
        print(f"\nLoaded pipelines: {list(pipelines_dict.keys())}")
        sentences, sim_matrix = example_sentence_embeddings()
        print("\n=== Sentence Embeddings Demo ===")
        print("Sentences:", sentences)
        print("Similarity Matrix:\n", sim_matrix)
        if "text_generation" in pipelines_dict:
            print("\n=== DistilGPT2 Text Generation Demo ===")
            prompt = "Once upon a time, in a land far away,"
            output = pipelines_dict["text_generation"](prompt, max_length=40, do_sample=True)
            print("Generated text:", output[0]["generated_text"])
        else:
            print("[text_generation] pipeline not loaded.")

    if args.ui:
        run_gradio_ui(pipelines_dict)
    elif not args.demo:
        print("Usage examples:")
        print("  python thann.py --demo  (run pipeline demos)")
        print("  python thann.py --ui    (launch Gradio UI)")

if __name__ == "__main__":
    main()


##############################################################################
# Timer Utility Class
##############################################################################

class Timer:
    def __init__(self):
        self.items = {}
        self.time_scale = 1000.0  # ms
        self.time_unit = "ms"

    def start(self, name: str) -> None:
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        self.items[name] = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else time.time()
        print(f"{name} ...")

    def end(self, name: str) -> float:
        if name not in self.items:
            return 0
        if torch.cuda.is_available():
            end_event = torch.cuda.Event(enable_timing=True)
            end_event.record()
            torch.cuda.synchronize()
            elapsed_time = self.items[name].elapsed_time(end_event)
            print(f"{name} finished in {elapsed_time:.2f}{self.time_unit}.")
            del self.items[name]
            return elapsed_time
        else:
            start_time = self.items.pop(name)
            delta = time.time() - start_time
            t = delta * self.time_scale
            print(f"{name} finished in {t:.2f}{self.time_unit}.")
            return t
