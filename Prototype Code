#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
thann.py
--------
A production-ready example of a Toroidal Hive (THANN) prototype.
This version demonstrates two key components:
  1. THANN Model Blueprint:
       - A conceptual ToroidalHiveModel (an LSTM with a feedback loop).
       - This blueprint represents ideas for self-healing, adaptive, and swarm-based learning.
  2. Multimodal Pipelines Using Hugging Face Models:
       - Pipelines for text generation, fill-mask, image classification, zero-shot image classification, and text-to-text generation.
       
Requirements:
  pip install torch transformers sentence-transformers
"""

import argparse
import torch
import numpy as np

# Hugging Face Transformers
from transformers import (
    pipeline,
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModelForMaskedLM,
    AutoImageProcessor,
    AutoModelForImageClassification,
    AutoProcessor,
    AutoModelForZeroShotImageClassification,
    AutoModelForSeq2SeqLM
)

# Sentence Transformers (for embeddings/similarity)
from sentence_transformers import SentenceTransformer

##############################################################################
# 1. THANN Model Blueprint: ToroidalHiveModel
##############################################################################

class ToroidalHiveModel(torch.nn.Module):
    """
    A conceptual neural network illustrating a 'toroidal' feedback loop.
    In an advanced THANN system, you would also include:
      - Self-healing or error correction layers
      - Emotional weighting or adaptive learning rates
      - Possibly quantum modules (e.g., using Qiskit)
      - Swarm-based multi-agent logic
    This simplified version implements an LSTM with a basic feedback mechanism.
    """
    def __init__(self, hidden_size=256, vocab_size=30522):
        super(ToroidalHiveModel, self).__init__()
        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=hidden_size)
        self.lstm = torch.nn.LSTM(hidden_size, hidden_size, batch_first=True)
        self.classifier = torch.nn.Linear(hidden_size, vocab_size)
        self.feedback_transform = torch.nn.Linear(vocab_size, hidden_size)

    def forward(self, input_ids, hidden=None):
        # 1. Get embeddings
        emb = self.embedding(input_ids)
        # 2. Process through LSTM
        output, (h_n, c_n) = self.lstm(emb, hidden)
        # 3. Produce logits
        logits = self.classifier(output)
        # 4. Create simple feedback (averaged softmax of the logits)
        feedback = torch.softmax(logits, dim=-1).mean(dim=1)
        feedback_hidden = self.feedback_transform(feedback)
        # (In a complete design, feedback_hidden could be reintroduced to steer learning.)
        return logits, (h_n, c_n)

##############################################################################
# 2. Multimodal Pipelines Using Smaller, Real Models
##############################################################################

def create_multimodal_pipelines():
    """
    Returns a dictionary of Hugging Face pipeline objects for:
      - Text Generation (using DistilGPT2)
      - Fill-Mask (using BERT)
      - Image Classification (using ViT)
      - Zero-Shot Image Classification (using CLIP)
      - Text2Text Generation (using FLAN-T5-Small)
    Each model is chosen to be relatively small.
    """
    pipelines_dict = {}

    # TEXT GENERATION using DistilGPT2
    try:
        text_gen_tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
        text_gen_model = AutoModelForCausalLM.from_pretrained("distilgpt2")
        pipelines_dict["text_generation"] = pipeline(
            "text-generation",
            model=text_gen_model,
            tokenizer=text_gen_tokenizer
        )
    except Exception as e:
        print(f"[Warning] Failed to load text-generation (distilgpt2): {e}")

    # FILL-MASK using BERT base
    try:
        fill_mask_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        fill_mask_model = AutoModelForMaskedLM.from_pretrained("bert-base-uncased")
        pipelines_dict["fill_mask"] = pipeline(
            "fill-mask",
            model=fill_mask_model,
            tokenizer=fill_mask_tokenizer
        )
    except Exception as e:
        print(f"[Warning] Failed to load fill-mask (bert-base-uncased): {e}")

    # IMAGE CLASSIFICATION using ViT base
    try:
        image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
        image_model = AutoModelForImageClassification.from_pretrained("google/vit-base-patch16-224")
        pipelines_dict["image_classification"] = pipeline(
            "image-classification",
            model=image_model,
            feature_extractor=image_processor
        )
    except Exception as e:
        print(f"[Warning] Failed to load image-classification (vit-base): {e}")

    # ZERO-SHOT IMAGE CLASSIFICATION using CLIP base
    try:
        zero_shot_processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")
        zero_shot_model = AutoModelForZeroShotImageClassification.from_pretrained("openai/clip-vit-base-patch32")
        pipelines_dict["zero_shot_image"] = pipeline(
            "zero-shot-image-classification",
            model=zero_shot_model,
            feature_extractor=zero_shot_processor
        )
    except Exception as e:
        print(f"[Warning] Failed to load zero-shot-image (clip-vit-base-patch32): {e}")

    # TEXT2TEXT GENERATION using FLAN-T5-Small
    try:
        text2text_tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")
        text2text_model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")
        pipelines_dict["text2text_generation"] = pipeline(
            "text2text-generation",
            model=text2text_model,
            tokenizer=text2text_tokenizer
        )
    except Exception as e:
        print(f"[Warning] Failed to load text2text-generation (flan-t5-small): {e}")

    return pipelines_dict

##############################################################################
# 3. Sentence Embeddings Example
##############################################################################

def example_sentence_embeddings():
    """
    Demonstrates how to compute embeddings with SentenceTransformer
    (all-MiniLM-L6-v2) and prints a dot-product similarity matrix.
    """
    st_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    sentences = [
        "That is a happy person",
        "That is a happy dog",
        "That is a very happy person",
        "Today is a sunny day"
    ]
    embeddings = st_model.encode(sentences)
    sim_matrix = np.inner(embeddings, embeddings)
    return sentences, sim_matrix

##############################################################################
# 4. Main Script for Demo
##############################################################################

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--demo", action="store_true", help="Run demonstration of pipelines and sentence embeddings.")
    args = parser.parse_args()

    if args.demo:
        # Demonstrate the multimodal pipelines
        pipelines_dict = create_multimodal_pipelines()
        print("\nLoaded pipelines:", list(pipelines_dict.keys()))

        # Sentence Embeddings Demo
        sentences, sim_matrix = example_sentence_embeddings()
        print("\n=== Sentence Embeddings Demo ===")
        print("Sentences:", sentences)
        print("Similarity Matrix:\n", sim_matrix)

        # Quick Text Generation Test
        if "text_generation" in pipelines_dict:
            print("\n=== DistilGPT2 Text Generation Demo ===")
            prompt = "Once upon a time, in a land far away,"
            output = pipelines_dict["text_generation"](prompt, max_length=40, do_sample=True)
            print("Generated text:", output[0]["generated_text"])
        else:
            print("[text_generation] pipeline not loaded.")
    else:
        print("Usage: python thann.py --demo")
        print("This demo shows the THANN model blueprint along with multimodal pipelines.")

if __name__ == "__main__":
    main()
