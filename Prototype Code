import torch
from transformers import AutoModel, BitsAndBytesConfig

# === 1. Slim Tokenization (34M) ===
class LearntTokenization(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.convs = torch.nn.Sequential(
            torch.nn.Conv1d(768, 384, 3),  # Reduced from 2048
            torch.nn.GELU(),
            torch.nn.Conv1d(384, 512, 3),
            torch.nn.Dropout(0.1)
        )
        self.attn = torch.nn.MultiheadAttention(512, 4)  # 4 heads

# === 2. Compact Backbone (760M) ===
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)
backbone = AutoModel.from_pretrained(
    "microsoft/phi-3-mini-4k-instruct", 
    quantization_config=bnb_config,
    device_map="auto"
)

# === 3. Efficient MoA (280M) ===
class MoA(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.agents = torch.nn.ModuleDict({
            "text": AutoModel.from_pretrained(
                "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                device_map="auto",
                load_in_4bit=True
            ),
            "vision": AutoModel.from_pretrained(
                "google/mobilenet_v2_1.0_224",
                num_classes=512
            )
        })
        self.gate = torch.nn.Linear(512, 2)  # Top-2 gating
        self.lora_adapter = torch.nn.Linear(1024, 512)  # LoRA fusion
        
    def forward(self, x):
        gate_scores = torch.topk(torch.softmax(self.gate(x), -1), k=2)
        outputs = [self.agents["text"](x), self.agents["vision"](x)]
        return self.lora_adapter(torch.cat(outputs, dim=-1))


