#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
thann.py
--------
A production-ready example of a Toroidal Hive (THANN) prototype using
smaller, reliable Hugging Face models for text, fill-mask, and image tasks.

New Features:
  1. ResNet (microsoft/resnet-50) pipeline for image classification.
  2. Gradio UI to demo text generation and ResNet-based image classification.

Requirements:
  pip install torch transformers sentence-transformers gradio
"""

import argparse
import torch
import numpy as np

# Hugging Face Transformers
from transformers import (
    pipeline,
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModelForMaskedLM,
    AutoImageProcessor,
    AutoModelForImageClassification,
    AutoProcessor,
    AutoModelForZeroShotImageClassification,
    AutoModelForSeq2SeqLM
)

# Sentence Transformers (for embeddings/similarity)
from sentence_transformers import SentenceTransformer

##############################################################################
# 1. Toroidal Hive Model (Conceptual Example)
##############################################################################

class ToroidalHiveModel(torch.nn.Module):
    """
    A conceptual neural network illustrating a 'toroidal' feedback loop.
    In a real THANN system, you'd add:
      - Self-healing or error correction layers
      - Emotional weighting or adaptive learning rates
      - Possibly quantum modules (if using Qiskit, etc.)
      - Swarm-based multi-agent logic

    Here, it's an LSTM with a simple feedback transform for demonstration.
    """
    def __init__(self, hidden_size=256, vocab_size=30522):
        super(ToroidalHiveModel, self).__init__()
        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=hidden_size)
        self.lstm = torch.nn.LSTM(hidden_size, hidden_size, batch_first=True)
        self.classifier = torch.nn.Linear(hidden_size, vocab_size)

        # Example of a feedback transformation for post-processing logits
        self.feedback_transform = torch.nn.Linear(vocab_size, hidden_size)

    def forward(self, input_ids, hidden=None):
        """
        Forward pass with a basic feedback loop:
        1) Input embeddings
        2) LSTM
        3) Classifier -> vocab logits
        4) Convert the average softmax logits back to hidden space
        """
        # 1. Embedding
        emb = self.embedding(input_ids)

        # 2. LSTM
        output, (h_n, c_n) = self.lstm(emb, hidden)

        # 3. Classifier
        logits = self.classifier(output)

        # 4. Simple feedback
        feedback = torch.softmax(logits, dim=-1).mean(dim=1)  # average over time dimension
        feedback_hidden = self.feedback_transform(feedback)
        # You could feed 'feedback_hidden' back into the LSTM or a specialized module.

        return logits, (h_n, c_n)


##############################################################################
# 2. Multimodal Pipelines Using Smaller, Real Models (with ResNet added)
##############################################################################

def create_multimodal_pipelines():
    """
    Returns a dictionary of Hugging Face pipeline objects for:
      - Text Generation (DistilGPT2)
      - Fill-Mask (BERT base)
      - Image Classification (ViT base)
      - Zero-Shot Image Classification (CLIP base)
      - Text2Text Generation (FLAN-T5-Small)
      - ResNet Image Classification (microsoft/resnet-50)
    Each is relatively small, so you avoid long downloads or timeouts.
    """
    pipelines_dict = {}

    # TEXT GENERATION: DistilGPT2 (~500 MB)
    try:
        text_gen_tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
        text_gen_model = AutoModelForCausalLM.from_pretrained("distilgpt2")
        pipelines_dict["text_generation"] = pipeline(
            "text-generation",
            model=text_gen_model,
            tokenizer=text_gen_tokenizer
        )
    except Exception as e:
        print(f"[Warning] Failed to load text-generation (distilgpt2): {e}")

    # FILL-MASK: BERT base (~420 MB)
    try:
        fill_mask_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        fill_mask_model = AutoModelForMaskedLM.from_pretrained("bert-base-uncased")
        pipelines_dict["fill_mask"] = pipeline(
            "fill-mask",
            model=fill_mask_model,
            tokenizer=fill_mask_tokenizer
        )
    except Exception as e:
        print(f"[Warning] Failed to load fill-mask (bert-base-uncased): {e}")

    # IMAGE CLASSIFICATION: ViT base (~330 MB)
    try:
        image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
        image_model = AutoModelForImageClassification.from_pretrained("google/vit-base-patch16-224")
        pipelines_dict["image_classification"] = pipeline(
            "image-classification",
            model=image_model,
            feature_extractor=image_processor
        )
    except Exception as e:
        print(f"[Warning] Failed to load image-classification (vit-base): {e}")

    # ZERO-SHOT IMAGE CLASSIFICATION: CLIP base (~500 MB)
    try:
        zero_shot_processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")
        zero_shot_model = AutoModelForZeroShotImageClassification.from_pretrained("openai/clip-vit-base-patch32")
        pipelines_dict["zero_shot_image"] = pipeline(
            "zero-shot-image-classification",
            model=zero_shot_model,
            feature_extractor=zero_shot_processor
        )
    except Exception as e:
        print(f"[Warning] Failed to load zero-shot-image (clip-vit-base-patch32): {e}")

    # TEXT2TEXT GENERATION: FLAN-T5-Small (~300 MB)
    try:
        text2text_tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")
        text2text_model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")
        pipelines_dict["text2text_generation"] = pipeline(
            "text2text-generation",
            model=text2text_model,
            tokenizer=text2text_tokenizer
        )
    except Exception as e:
        print(f"[Warning] Failed to load text2text-generation (flan-t5-small): {e}")

    # RESNET IMAGE CLASSIFICATION: microsoft/resnet-50
    try:
        resnet_processor = AutoImageProcessor.from_pretrained("microsoft/resnet-50")
        resnet_model = AutoModelForImageClassification.from_pretrained("microsoft/resnet-50")
        pipelines_dict["resnet_classification"] = pipeline(
            "image-classification",
            model=resnet_model,
            feature_extractor=resnet_processor
        )
    except Exception as e:
        print(f"[Warning] Failed to load resnet classification (microsoft/resnet-50): {e}")

    return pipelines_dict


##############################################################################
# 3. Sentence Embeddings Example
##############################################################################

def example_sentence_embeddings():
    """
    Demonstrates how to compute embeddings with SentenceTransformer
    (all-MiniLM-L6-v2) and produce a simple dot-product similarity matrix.
    """
    model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    sentences = [
        "That is a happy person",
        "That is a happy dog",
        "That is a very happy person",
        "Today is a sunny day"
    ]
    embeddings = model.encode(sentences)
    # Dot product similarity matrix
    sim_matrix = np.inner(embeddings, embeddings)
    return sentences, sim_matrix


##############################################################################
# 4. Gradio UI
##############################################################################

def run_gradio_ui(pipelines_dict):
    """
    Launches a simple Gradio app demonstrating:
      1) Text generation with DistilGPT2 (if loaded).
      2) ResNet-50 image classification (if loaded).
    """
    import gradio as gr

    # Text generation function
    def generate_text(prompt):
        if "text_generation" not in pipelines_dict:
            return "Text generation pipeline not loaded."
        result = pipelines_dict["text_generation"](prompt, max_length=40, do_sample=True)
        return result[0]["generated_text"]

    # ResNet image classification function
    def classify_image(image):
        if "resnet_classification" not in pipelines_dict:
            return "ResNet classification pipeline not loaded."
        results = pipelines_dict["resnet_classification"](image)
        return results

    with gr.Blocks() as demo:
        gr.Markdown("# THANN Demo: Text Generation & ResNet Classification")

        with gr.Tab("Text Generation"):
            text_input = gr.Textbox(lines=3, label="Enter your prompt:")
            text_output = gr.Textbox(label="Generated Text")
            generate_button = gr.Button("Generate")
            generate_button.click(fn=generate_text, inputs=text_input, outputs=text_output)

        with gr.Tab("ResNet Image Classification"):
            image_input = gr.Image(label="Upload an image", type="pil")
            image_output = gr.JSON(label="Classification Results")
            classify_button = gr.Button("Classify")
            classify_button.click(fn=classify_image, inputs=image_input, outputs=image_output)

        demo.launch()


##############################################################################
# 5. Main Script
##############################################################################

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--demo", action="store_true",
                        help="Run demonstration: pipelines + embeddings.")
    parser.add_argument("--ui", action="store_true",
                        help="Launch Gradio UI for text & image classification.")
    args = parser.parse_args()

    if args.demo or args.ui:
        # Create pipelines
        pipelines_dict = create_multimodal_pipelines()

    if args.demo:
        # 1) Print loaded pipelines
        print(f"\nLoaded pipelines: {list(pipelines_dict.keys())}")

        # 2) Sentence embeddings example
        sentences, sim_matrix = example_sentence_embeddings()
        print("\n=== Sentence Embeddings Demo ===")
        print("Sentences:", sentences)
        print("Similarity Matrix:\n", sim_matrix)

        # 3) Quick text generation test (if pipeline loaded)
        if "text_generation" in pipelines_dict:
            print("\n=== DistilGPT2 Text Generation Demo ===")
            prompt = "Once upon a time, in a land far away,"
            output = pipelines_dict["text_generation"](
                prompt,
                max_length=40,
                do_sample=True
            )
            print("Generated text:", output[0]["generated_text"])
        else:
            print("[text_generation] pipeline not loaded.")
    
    if args.ui:
        # Launch the Gradio UI for text and image classification
        run_gradio_ui(pipelines_dict)
    else:
        if not args.demo:
            print("Usage examples:")
            print("  python thann.py --demo  (run pipeline demos)")
            print("  python thann.py --ui    (launch Gradio UI)")


if __name__ == "__main__":
    main()
